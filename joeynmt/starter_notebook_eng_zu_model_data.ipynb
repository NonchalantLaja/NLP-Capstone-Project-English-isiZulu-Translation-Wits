{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "starter_notebook-eng-zu-model-data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "## Note before beginning:\n",
        "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "\n",
        "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "\n",
        "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "\n",
        "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
        "\n",
        "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
        "\n",
        "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "### - This notebook is intended to be used with custom parallel data. That means that you need two files, where one is in your language, the other English, and the lines in the files are corresponding translations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Pre-process your data\n",
        "\n",
        "We assume here that you already have a data set. The format in which we will process it here requires that \n",
        "1. you have two files, one for each language\n",
        "2. the files are sentence-aligned, which means that each line should correspond to the same line in the other file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGRmDELn7Az0",
        "outputId": "b47bea5c-af7c-4166-f76d-c4d823742ba0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn3tgQLzUxwn"
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"zu\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/parallel_corpus/baseline/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/parallel_corpus/baseline/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBSgJHEw7Nvx",
        "outputId": "88c7b58a-7d91-444a-c372-c16258bec969"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/parallel_corpus/baseline/en-zu-baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA75Fs9ys8Y9",
        "outputId": "7f070d21-9eda-4748-f408-ef12975e8a5f"
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opustools-pkg in /usr/local/lib/python3.7/dist-packages (0.0.52)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xq-tDZVks7ZD",
        "outputId": "1fc22d12-9b6b-4764-9e17-22974e4c4552"
      },
      "source": [
        "# TODO: specify the file paths here\n",
        "source_file = \"/content/drive/My Drive/parallel_corpus/my_en_eval.txt\"\n",
        "target_file = \"/content/drive/My Drive/parallel_corpus/my_zu_eval_998.txt\"\n",
        "\n",
        "# They should both have the same length.\n",
        "! wc -l \"$source_file\"\n",
        "! wc -l \"$target_file\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "997 /content/drive/My Drive/parallel_corpus/my_en_eval.txt\n",
            "997 /content/drive/My Drive/parallel_corpus/my_zu_eval_998.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OidA5gpe2mqA",
        "outputId": "76e54449-0ecf-423b-851f-11e35885096a"
      },
      "source": [
        "# TODO: Pre-processing! (OPTIONAL)\n",
        "\n",
        "# If your data contains weird symbols or the like, you might want to do some cleaning and normalization.\n",
        "# We don't have the code in the notebook for that, but you can use sacremoses \"normalize\" for example for normalization punctuation: https://github.com/alvations/sacremoses.\n",
        "\n",
        "# We apply tokenization to separate punctuation marks from the actual words, split words at hyphens etc.\n",
        "# If you're data is already tokenized, that's great! Skip this cell.\n",
        "# Otherwise we can use sacremoses to do the tokenization for us. \n",
        "# We need the data to be tokenized such that it matches the global test set.\n",
        "\n",
        "! pip install sacremoses\n",
        "\n",
        "tok_source_file = source_file+\".tok\"\n",
        "tok_target_file = target_file+\".tok\"\n",
        "\n",
        "# Tokenize the source\n",
        "! sacremoses -l \"$source_language\" tokenize < \"$source_file\" > \"$tok_source_file\"\n",
        "# Tokenize the target\n",
        "! sacremoses -l \"$target_language\" tokenize < \"$target_file\" > \"$tok_target_file\"\n",
        "\n",
        "# Let's take a look what tokenization did to the text.\n",
        "! head \"$source_file\"*\n",
        "! head \"$target_file\"*\n",
        "\n",
        "# Change the pointers to our files such that we continue to work with the tokenized data.\n",
        "source_file = tok_source_file\n",
        "target_file = tok_target_file"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.46)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.62.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "==> /content/drive/My Drive/parallel_corpus/my_en_eval.txt <==\n",
            "Peter Van Sant: And it means what?\n",
            "The cost to society will be substantial, the report says. In 2019 alone, it estimates a $290 billion burden from health care, long-term case and hospice combined. Medicare and Medicaid will cover $195 billion of that, with out-of-pocket costs to caregivers reaching $63 billion.\n",
            "It is now up to them to make the most of it.\n",
            "\"\"\"The CPS is carefully considering all the available information, including the impact on Harry's family, in order to make an independent and objective charging decision.\"\n",
            "TV audiences were left outraged after a teenage girl appeared on Channel 4's 24 Hours in A&E complaining of a broken finger nail.\n",
            "The disgusting note read: 'Put your dog on a lead, slag!\n",
            "Sen Sen CEO Subhash Challa\n",
            "Someone behind the camera says: 'Hi guys, hi!\n",
            "\"\"\"She was devoted to children and especially animals, including a wild fox who we are continuing to feed now that she has gone.\"\"\"\n",
            "Are there literally ... no other actors alive?\n",
            "\n",
            "==> /content/drive/My Drive/parallel_corpus/my_en_eval.txt.tok <==\n",
            "Peter Van Sant : And it means what ?\n",
            "The cost to society will be substantial , the report says . In 2019 alone , it estimates a $ 290 billion burden from health care , long-term case and hospice combined . Medicare and Medicaid will cover $ 195 billion of that , with out-of-pocket costs to caregivers reaching $ 63 billion .\n",
            "It is now up to them to make the most of it .\n",
            "&quot; &quot; &quot; The CPS is carefully considering all the available information , including the impact on Harry &apos;s family , in order to make an independent and objective charging decision . &quot;\n",
            "TV audiences were left outraged after a teenage girl appeared on Channel 4 &apos;s 24 Hours in A &amp; E complaining of a broken finger nail .\n",
            "The disgusting note read : &apos; Put your dog on a lead , slag !\n",
            "Sen Sen CEO Subhash Challa\n",
            "Someone behind the camera says : &apos; Hi guys , hi !\n",
            "&quot; &quot; &quot; She was devoted to children and especially animals , including a wild fox who we are continuing to feed now that she has gone . &quot; &quot; &quot;\n",
            "Are there literally ... no other actors alive ?\n",
            "==> /content/drive/My Drive/parallel_corpus/my_zu_eval_998.txt <==\n",
            "Peter Van Sant: Bese kusho ukuthini?\n",
            "Izindleko zomphakathi zizoba zinkulu, ngokusho kombiko. Ngonyaka wezi-2019 uwodwa, umthwalo wezindleko zezempilo ulinganiselwa kuzigidigidi ezingama-$290, kuhlanganiswa ukugula kwesikhathi eside kanye ne-hospice.  I-Medicare ne-Medicaid izokhokhela izindleko eziyizigidigidi ezingama-$195 walokho, ngezindleko ezikhokhwa ngqo zabanakekeli ezifinyelela kuzigidigidi ezingama-$63.\n",
            "Sekulele kubo ukuthi bayisebenzise ngendlela ezoba nenzuzo kakhulu.\n",
            "\"\"\"I-CPS icubungulisisa lonke ulwazi olukhona, okubandakanya umthelela emndenini kaHarry, ukuze kuthathwe isinqumo sokukhokhisa esizimele nesingachemile.\"\n",
            "Izibukeli zaku-TV zishiywe zimangele emuva kokuba kuvele intombazane esakhula ohlelweni i-24 Hours ku-A&E kuShaneli 4 ikhalaza ngokuphukelwa wuzipho.\n",
            "Umbhalo onyanyisayo ubufundeka kanje: 'Beka inja yakho phambili, slag!\n",
            "USen Sen u-CEO weSubhash Challa\n",
            "Kunomuntu ongemuva kwekhamera othi: Sanibonani bafowethu, sanibonani!\n",
            "\"\"\"Wayezinikele ezinganeni ikhalukazi nasezilwaneni, okubandakanya impungushe yasendle esiqhubekayo nokuyiphakela namanje engasekho.\"\"\"\n",
            "Ingabe ngempela ... abasekho nhlobo abanye abalingisi abasaphila?\n",
            "\n",
            "==> /content/drive/My Drive/parallel_corpus/my_zu_eval_998.txt.tok <==\n",
            "Peter Van Sant : Bese kusho ukuthini ?\n",
            "Izindleko zomphakathi zizoba zinkulu , ngokusho kombiko . Ngonyaka wezi-2019 uwodwa , umthwalo wezindleko zezempilo ulinganiselwa kuzigidigidi ezingama- $ 290 , kuhlanganiswa ukugula kwesikhathi eside kanye ne-hospice . I-Medicare ne-Medicaid izokhokhela izindleko eziyizigidigidi ezingama- $ 195 walokho , ngezindleko ezikhokhwa ngqo zabanakekeli ezifinyelela kuzigidigidi ezingama- $ 63 .\n",
            "Sekulele kubo ukuthi bayisebenzise ngendlela ezoba nenzuzo kakhulu .\n",
            "&quot; &quot; &quot; I-CPS icubungulisisa lonke ulwazi olukhona , okubandakanya umthelela emndenini kaHarry , ukuze kuthathwe isinqumo sokukhokhisa esizimele nesingachemile . &quot;\n",
            "Izibukeli zaku-TV zishiywe zimangele emuva kokuba kuvele intombazane esakhula ohlelweni i-24 Hours ku-A &amp; E kuShaneli 4 ikhalaza ngokuphukelwa wuzipho .\n",
            "Umbhalo onyanyisayo ubufundeka kanje : &apos; Beka inja yakho phambili , slag !\n",
            "USen Sen u-CEO weSubhash Challa\n",
            "Kunomuntu ongemuva kwekhamera othi : Sanibonani bafowethu , sanibonani !\n",
            "&quot; &quot; &quot; Wayezinikele ezinganeni ikhalukazi nasezilwaneni , okubandakanya impungushe yasendle esiqhubekayo nokuyiphakela namanje engasekho . &quot; &quot; &quot;\n",
            "Ingabe ngempela ... abasekho nhlobo abanye abalingisi abasaphila ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n48GDRnP8y2G",
        "outputId": "808a6f89-ddc5-4ca8-a077-08a733ceaa51"
      },
      "source": [
        "# Download the global test set.\n",
        "! wget https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language \n",
        "\n",
        "! wget https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "! mv test.en-$trg.en test.en\n",
        "! wget https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "! mv test.en-$trg.$trg test.$trg\n",
        "\n",
        "# TODO: if this fails it means that there is NO test set for your language yet. It's on you to create one.\n",
        "# A good idea would be to take a random subset of your data, and add it to https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-any.en.\n",
        "# Make a Pull Request and get it approved and merged.\n",
        "# Then repeat this cell to retrieve the new test set.\n",
        "# Then proceed to the next cell that will filter out all duplicates from the training set, so that there is no overlap between training and test set."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-20 16:50:21--  https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-any.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277791 (271K) [text/plain]\n",
            "Saving to: â€˜test.en-any.en.2â€™\n",
            "\n",
            "test.en-any.en.2    100%[===================>] 271.28K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-11-20 16:50:21 (8.73 MB/s) - â€˜test.en-any.en.2â€™ saved [277791/277791]\n",
            "\n",
            "--2021-11-20 16:50:21--  https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-zu.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206207 (201K) [text/plain]\n",
            "Saving to: â€˜test.en-zu.enâ€™\n",
            "\n",
            "test.en-zu.en       100%[===================>] 201.37K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-11-20 16:50:21 (8.30 MB/s) - â€˜test.en-zu.enâ€™ saved [206207/206207]\n",
            "\n",
            "--2021-11-20 16:50:21--  https://raw.githubusercontent.com/masakhane-io/masakhane/master/jw300_utils/test/test.en-zu.zu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 218273 (213K) [text/plain]\n",
            "Saving to: â€˜test.en-zu.zuâ€™\n",
            "\n",
            "test.en-zu.zu       100%[===================>] 213.16K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-11-20 16:50:22 (8.32 MB/s) - â€˜test.en-zu.zuâ€™ saved [218273/218273]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqDG-CI28y2L",
        "outputId": "f763f944-65de-4885-b283-e8bb22580d61"
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = \"test.en-any.en\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3571 global test sentences to filter from the training/dev data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "3CNdwLBCfSIl",
        "outputId": "ca522799-a4f3-4a22-f8ed-18cab2f06999"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        if line.strip() not in en_test_sents:\n",
        "            source.append(line.strip())\n",
        "        else:\n",
        "            skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        if j not in skip_lines:\n",
        "            target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data and skipped 0/997 lines since contained in test set.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Peter Van Sant: And it means what?</td>\n",
              "      <td>Peter Van Sant: Bese kusho ukuthini?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The cost to society will be substantial, the r...</td>\n",
              "      <td>Izindleko zomphakathi zizoba zinkulu, ngokusho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It is now up to them to make the most of it.</td>\n",
              "      <td>Sekulele kubo ukuthi bayisebenzise ngendlela e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     source_sentence                                    target_sentence\n",
              "0                 Peter Van Sant: And it means what?               Peter Van Sant: Bese kusho ukuthini?\n",
              "1  The cost to society will be substantial, the r...  Izindleko zomphakathi zizoba zinkulu, ngokusho...\n",
              "2       It is now up to them to make the most of it.  Sekulele kubo ukuthi bayisebenzise ngendlela e..."
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_2ouEOH1_1q"
      },
      "source": [
        "# DOnt  RUN\n",
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i5-mI7M2mqL",
        "outputId": "a464c42b-f9bd-4f80-fe21-aba254609f04"
      },
      "source": [
        "# DOnt RUN\n",
        "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
        "# test and training sets.\n",
        "! pip install fuzzywuzzy\n",
        "! pip install python-Levenshtein\n",
        "import time\n",
        "from fuzzywuzzy import process\n",
        "import numpy as np\n",
        "from os import cpu_count\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "\n",
        "\n",
        "# reset the index of the training set after previous filtering\n",
        "df_pp.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Remove samples from the training data set if they \"almost overlap\" with the\n",
        "# samples in the test set.\n",
        "\n",
        "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
        "# within a certain length of characters of the given sample.\n",
        "def fuzzfilter(sample, candidates, pad):\n",
        "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
        "  if len(candidates) > 0:\n",
        "    return process.extractOne(sample, candidates)[1]\n",
        "  else:\n",
        "    return np.nan"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "dpa-Jomq2mqN",
        "outputId": "89e8c22b-9398-497e-e692-f748cdf954d7"
      },
      "source": [
        "# Dont RUN\n",
        "start_time = time.time()\n",
        "### iterating over pandas dataframe rows is not recomended, let use multi processing to apply the function\n",
        "\n",
        "with Pool(cpu_count()-1) as pool:\n",
        "    scores = pool.map(partial(fuzzfilter, candidates=list(en_test_sents), pad=5), df_pp['source_sentence'])\n",
        "hours, rem = divmod(time.time() - start_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(\"done in {}h:{}min:{}seconds\".format(hours, minutes, seconds))\n",
        "\n",
        "# Filter out \"almost overlapping samples\"\n",
        "df_pp = df_pp.assign(scores=scores)\n",
        "df_pp = df_pp[df_pp['scores'] < 95]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3756b9872afa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### iterating over pandas dataframe rows is not recomended, let use multi processing to apply the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzzfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_test_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_pp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Pool' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "hxxBOCA-xXhy",
        "outputId": "d2e0014e-9650-4583-d107-18f2abec5558"
      },
      "source": [
        "# Dont Run\n",
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# TODO: if your corpus is smaller than 1000, reduce this number. With a corpus that small you might not obtain good results with NMT though :/\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 100\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "\n",
        "# TODO: Doublecheck the format below. There should be no extra quotation marks or weird characters. It should also not be empty.\n",
        "! head train.*\n",
        "! head dev.*"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-951a8d7ca721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Julia: test sets are already generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_dev_patterns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Herman: Error in original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mstripped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_dev_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_pp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd5KPDTKQ_-4",
        "outputId": "6ff7f042-a4dd-4375-cf7d-ef6368bb758b"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# TODO: if your corpus is smaller than 1000, reduce this number. With a corpus that small you might not obtain good results with NMT though :/\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 100\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df[\"source_sentence\"] = df[\"source_sentence\"].str.lower()\n",
        "    df[\"target_sentence\"] = df[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df.drop(df.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "\n",
        "# TODO: Doublecheck the format below. There should be no extra quotation marks or weird characters. It should also not be empty.\n",
        "! head train.*\n",
        "! head dev.*"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> train.bpe.en <==\n",
            "\n",
            "==> train.bpe.zu <==\n",
            "\n",
            "==> train.en <==\n",
            "Peter Van Sant: And it means what?\n",
            "The cost to society will be substantial, the report says. In 2019 alone, it estimates a $290 billion burden from health care, long-term case and hospice combined. Medicare and Medicaid will cover $195 billion of that, with out-of-pocket costs to caregivers reaching $63 billion.\n",
            "It is now up to them to make the most of it.\n",
            "\"\"\"The CPS is carefully considering all the available information, including the impact on Harry's family, in order to make an independent and objective charging decision.\"\n",
            "TV audiences were left outraged after a teenage girl appeared on Channel 4's 24 Hours in A&E complaining of a broken finger nail.\n",
            "The disgusting note read: 'Put your dog on a lead, slag!\n",
            "Sen Sen CEO Subhash Challa\n",
            "Someone behind the camera says: 'Hi guys, hi!\n",
            "\"\"\"She was devoted to children and especially animals, including a wild fox who we are continuing to feed now that she has gone.\"\"\"\n",
            "Are there literally ... no other actors alive?\n",
            "\n",
            "==> train.zu <==\n",
            "Peter Van Sant: Bese kusho ukuthini?\n",
            "Izindleko zomphakathi zizoba zinkulu, ngokusho kombiko. Ngonyaka wezi-2019 uwodwa, umthwalo wezindleko zezempilo ulinganiselwa kuzigidigidi ezingama-$290, kuhlanganiswa ukugula kwesikhathi eside kanye ne-hospice.  I-Medicare ne-Medicaid izokhokhela izindleko eziyizigidigidi ezingama-$195 walokho, ngezindleko ezikhokhwa ngqo zabanakekeli ezifinyelela kuzigidigidi ezingama-$63.\n",
            "Sekulele kubo ukuthi bayisebenzise ngendlela ezoba nenzuzo kakhulu.\n",
            "\"\"\"I-CPS icubungulisisa lonke ulwazi olukhona, okubandakanya umthelela emndenini kaHarry, ukuze kuthathwe isinqumo sokukhokhisa esizimele nesingachemile.\"\n",
            "Izibukeli zaku-TV zishiywe zimangele emuva kokuba kuvele intombazane esakhula ohlelweni i-24 Hours ku-A&E kuShaneli 4 ikhalaza ngokuphukelwa wuzipho.\n",
            "Umbhalo onyanyisayo ubufundeka kanje: 'Beka inja yakho phambili, slag!\n",
            "USen Sen u-CEO weSubhash Challa\n",
            "Kunomuntu ongemuva kwekhamera othi: Sanibonani bafowethu, sanibonani!\n",
            "\"\"\"Wayezinikele ezinganeni ikhalukazi nasezilwaneni, okubandakanya impungushe yasendle esiqhubekayo nokuyiphakela namanje engasekho.\"\"\"\n",
            "Ingabe ngempela ... abasekho nhlobo abanye abalingisi abasaphila?\n",
            "==> dev.bpe.en <==\n",
            "P@@ S - T@@ h@@ e@@ y h@@ a@@ v@@ e b@@ e@@ e@@ n w@@ a@@ r@@ n@@ e@@ d b@@ u@@ t t@@ h@@ e@@ y w@@ o@@ n &@@ a@@ p@@ o@@ s@@ ;@@ t s@@ t@@ o@@ p .\n",
            "M@@ y h@@ u@@ s@@ b@@ a@@ n@@ d w@@ a@@ s a@@ f@@ r@@ a@@ i@@ d h@@ i@@ s c@@ a@@ r m@@ i@@ g@@ h@@ t b@@ l@@ o@@ w a@@ w@@ a@@ y a@@ n@@ d m@@ y c@@ a@@ r i@@ s i@@ n t@@ h@@ e g@@ a@@ r@@ a@@ g@@ e . ðŸ¤¦ â€ â™€ ï¸ # H@@ u@@ r@@ r@@ i@@ c@@ a@@ n@@ e@@ D@@ o@@ r@@ i@@ a@@ n # D@@ o@@ r@@ i@@ a@@ n # F@@ l@@ o@@ G@@ r@@ o@@ w@@ n h@@ t@@ t@@ p@@ s : / / y@@ o@@ u@@ t@@ u@@ .@@ b@@ e / g@@ N@@ 6@@ L@@ o@@ 1@@ V@@ I@@ t@@ T@@ s\n",
            "T@@ h@@ a@@ n@@ k Y@@ o@@ u f@@ o@@ r S@@ m@@ o@@ k@@ i@@ n@@ g ( 2@@ 0@@ 0@@ 5 ) â˜… â˜… â˜… A@@ a@@ r@@ o@@ n E@@ c@@ k@@ h@@ a@@ r@@ t , M@@ a@@ r@@ i@@ a B@@ e@@ l@@ l@@ o . A l@@ o@@ b@@ b@@ y@@ i@@ s@@ t f@@ o@@ r b@@ i@@ g t@@ o@@ b@@ a@@ c@@ c@@ o f@@ i@@ n@@ d@@ s i@@ t d@@ i@@ f@@ f@@ i@@ c@@ u@@ l@@ t t@@ o b@@ a@@ l@@ a@@ n@@ c@@ e h@@ i@@ s d@@ u@@ t@@ i@@ e@@ s d@@ e@@ f@@ e@@ n@@ d@@ i@@ n@@ g a d@@ a@@ n@@ g@@ e@@ r@@ o@@ u@@ s s@@ u@@ b@@ s@@ t@@ a@@ n@@ c@@ e w@@ i@@ t@@ h t@@ h@@ o@@ s@@ e o@@ f b@@ e@@ i@@ n@@ g a g@@ o@@ o@@ d r@@ o@@ l@@ e m@@ o@@ d@@ e@@ l f@@ o@@ r h@@ i@@ s y@@ o@@ u@@ n@@ g s@@ o@@ n . ( R ) 1 h@@ r . 3@@ 2 m@@ i@@ n@@ s . H@@ B@@ O F@@ r@@ i . 1 : 3@@ 5 a@@ .@@ m@@ .\n",
            "A l@@ a@@ c@@ k o@@ f s@@ t@@ a@@ f@@ f t@@ o s@@ u@@ p@@ e@@ r@@ v@@ i@@ s@@ e t@@ h@@ e r@@ e@@ l@@ e@@ a@@ s@@ e o@@ f i@@ n@@ m@@ a@@ t@@ e@@ s f@@ r@@ o@@ m c@@ e@@ l@@ l@@ s t@@ o e@@ x@@ e@@ r@@ c@@ i@@ s@@ e a@@ n@@ d t@@ a@@ k@@ e p@@ a@@ r@@ t i@@ n o@@ t@@ h@@ e@@ r a@@ c@@ t@@ i@@ v@@ i@@ t@@ i@@ e@@ s h@@ a@@ s b@@ e@@ e@@ n l@@ i@@ n@@ k@@ e@@ d t@@ o a s@@ u@@ r@@ g@@ e i@@ n d@@ i@@ s@@ r@@ u@@ p@@ t@@ i@@ v@@ e b@@ e@@ h@@ a@@ v@@ i@@ o@@ u@@ r , v@@ i@@ o@@ l@@ e@@ n@@ c@@ e a@@ n@@ d s@@ e@@ l@@ f@@ -@@ h@@ a@@ r@@ m i@@ n B@@ r@@ i@@ t@@ a@@ i@@ n &@@ a@@ p@@ o@@ s@@ ;@@ s p@@ r@@ i@@ s@@ o@@ n@@ s .\n",
            "I@@ n t@@ h@@ e 1@@ 9@@ 6@@ 0@@ s , W@@ o@@ o@@ l@@ w@@ o@@ r@@ t@@ h@@ s b@@ o@@ u@@ g@@ h@@ t t@@ h@@ e 6@@ 5@@ -@@ s@@ t@@ r@@ o@@ n@@ g S@@ y@@ d@@ n@@ e@@ y c@@ h@@ a@@ i@@ n i@@ n@@ c@@ l@@ u@@ d@@ i@@ n@@ g t@@ h@@ e s@@ t@@ o@@ r@@ e t@@ h@@ a@@ t e@@ x@@ i@@ s@@ t@@ s t@@ o t@@ h@@ i@@ s d@@ a@@ y .\n",
            "W@@ h@@ a@@ t o@@ t@@ h@@ e@@ r b@@ r@@ a@@ n@@ d@@ s h@@ a@@ v@@ e y@@ o@@ u b@@ e@@ e@@ n i@@ n@@ v@@ o@@ l@@ v@@ e@@ d w@@ i@@ t@@ h o@@ v@@ e@@ r t@@ h@@ e c@@ o@@ u@@ r@@ s@@ e o@@ f y@@ o@@ u@@ r c@@ a@@ r@@ e@@ e@@ r ? D@@ i@@ d@@ n &@@ a@@ p@@ o@@ s@@ ;@@ t y@@ o@@ u d@@ o a K@@ a@@ t@@ e S@@ p@@ a@@ d@@ e N@@ e@@ w Y@@ o@@ r@@ k b@@ e@@ a@@ u@@ t@@ y c@@ a@@ m@@ p@@ a@@ i@@ g@@ n n@@ o@@ t t@@ o@@ o l@@ o@@ n@@ g a@@ g@@ o ?\n",
            "R@@ a@@ u@@ t h@@ a@@ d c@@ a@@ m@@ p@@ a@@ i@@ g@@ n@@ e@@ d a@@ g@@ a@@ i@@ n@@ s@@ t t@@ h@@ e s@@ t@@ a@@ t@@ e , a@@ c@@ c@@ u@@ s@@ i@@ n@@ g i@@ t o@@ f r@@ a@@ c@@ i@@ a@@ l d@@ i@@ s@@ c@@ r@@ i@@ m@@ i@@ n@@ a@@ t@@ i@@ o@@ n a@@ g@@ a@@ i@@ n@@ s@@ t M@@ a@@ d@@ h@@ e@@ s@@ i p@@ e@@ o@@ p@@ l@@ e .\n",
            "D@@ u@@ s@@ t@@ i@@ n B@@ i@@ l@@ y@@ k , a w@@ r@@ i@@ t@@ e@@ r f@@ r@@ o@@ m C@@ a@@ n@@ a@@ d@@ a , t@@ o@@ l@@ d h@@ o@@ w h@@ e w@@ a@@ s &@@ a@@ p@@ o@@ s@@ ; a@@ l@@ r@@ e@@ a@@ d@@ y a b@@ r@@ o@@ k@@ e@@ n m@@ a@@ n &@@ a@@ p@@ o@@ s@@ ; b@@ y t@@ h@@ e t@@ i@@ m@@ e h@@ i@@ s f@@ o@@ r@@ m@@ e@@ r w@@ i@@ f@@ e m@@ a@@ d@@ e t@@ h@@ e a@@ d@@ m@@ i@@ s@@ s@@ i@@ o@@ n , c@@ l@@ a@@ i@@ m@@ i@@ n@@ g t@@ h@@ a@@ t s@@ h@@ e h@@ a@@ d b@@ e@@ e@@ n &@@ a@@ p@@ o@@ s@@ ; f@@ a@@ k@@ i@@ n@@ g t@@ h@@ e m@@ a@@ r@@ r@@ i@@ a@@ g@@ e f@@ o@@ r t@@ w@@ o y@@ e@@ a@@ r@@ s &@@ a@@ p@@ o@@ s@@ ; .\n",
            "H@@ e s@@ a@@ i@@ d t@@ h@@ a@@ t h@@ i@@ s &@@ a@@ p@@ o@@ s@@ ; s@@ t@@ o@@ n@@ k@@ i@@ n@@ g &@@ a@@ p@@ o@@ s@@ ; w@@ i@@ n h@@ a@@ s g@@ i@@ v@@ e@@ n h@@ i@@ m &@@ a@@ p@@ o@@ s@@ ; a p@@ o@@ w@@ e@@ r@@ f@@ u@@ l m@@ a@@ n@@ d@@ a@@ t@@ e t@@ o g@@ e@@ t B@@ r@@ e@@ x@@ i@@ t d@@ o@@ n@@ e &@@ a@@ p@@ o@@ s@@ ;@@ C@@ r@@ e@@ d@@ i@@ t : G@@ e@@ t@@ t@@ y I@@ m@@ a@@ g@@ e@@ s - G@@ e@@ t@@ t@@ y\n",
            "J@@ o@@ e R@@ o@@ o@@ t ( c ) - Y@@ o@@ r@@ k@@ s@@ h@@ i@@ r@@ e\n",
            "\n",
            "==> dev.bpe.zu <==\n",
            "P@@ S â€“ B@@ a@@ x@@ w@@ a@@ y@@ i@@ s@@ i@@ w@@ e k@@ o@@ d@@ w@@ a n@@ g@@ e@@ k@@ e b@@ a@@ y@@ e@@ k@@ e .\n",
            "U@@ m@@ y@@ e@@ n@@ i w@@ a@@ m@@ i u@@ b@@ e@@ s@@ a@@ b@@ a u@@ k@@ u@@ t@@ h@@ i i@@ m@@ o@@ t@@ o i@@ n@@ g@@ a@@ s@@ e i@@ p@@ h@@ e@@ p@@ h@@ u@@ k@@ e f@@ u@@ t@@ h@@ i i@@ m@@ o@@ t@@ o y@@ a@@ m@@ i i@@ b@@ i@@ s@@ e@@ g@@ a@@ r@@ a@@ j@@ i . ðŸ¤¦ â€ â™€ ï¸ # H@@ u@@ r@@ r@@ i@@ c@@ a@@ n@@ e@@ D@@ o@@ r@@ i@@ a@@ n # D@@ o@@ r@@ i@@ a@@ n # F@@ l@@ o@@ G@@ r@@ o@@ w@@ n h@@ t@@ t@@ p@@ s : / / y@@ o@@ u@@ t@@ u@@ .@@ b@@ e / g@@ N@@ 6@@ L@@ o@@ 1@@ V@@ I@@ t@@ T@@ s\n",
            "I@@ -@@ T@@ h@@ a@@ n@@ k Y@@ o@@ u f@@ o@@ r S@@ m@@ o@@ k@@ i@@ n@@ g ( 2@@ 0@@ 0@@ 5 ) â˜… â˜… â˜… A@@ a@@ r@@ o@@ n E@@ c@@ k@@ h@@ a@@ r@@ t , M@@ a@@ r@@ i@@ a B@@ e@@ l@@ l@@ o . O@@ b@@ e@@ n@@ x@@ e@@ n@@ x@@ e@@ l@@ a u@@ g@@ w@@ a@@ y@@ i o@@ m@@ k@@ h@@ u@@ l@@ u u@@ t@@ h@@ o@@ l@@ a u@@ b@@ u@@ n@@ z@@ i@@ m@@ a b@@ o@@ k@@ u@@ l@@ i@@ n@@ g@@ a@@ n@@ i@@ s@@ a i@@ m@@ i@@ s@@ e@@ b@@ e@@ n@@ z@@ i y@@ a@@ k@@ h@@ e e@@ v@@ i@@ k@@ e@@ l@@ a u@@ k@@ u@@ s@@ e@@ b@@ e@@ n@@ z@@ i@@ s@@ a o@@ k@@ u@@ n@@ o@@ b@@ u@@ n@@ g@@ o@@ z@@ i e@@ k@@ u@@ b@@ e@@ n@@ i n@@ g@@ u@@ m@@ u@@ n@@ t@@ u o@@ w@@ e@@ n@@ z@@ a o@@ k@@ u@@ h@@ l@@ e i@@ n@@ d@@ o@@ d@@ a@@ n@@ a y@@ a@@ k@@ h@@ e e@@ n@@ g@@ a@@ b@@ u@@ k@@ e@@ l@@ a k@@ u@@ y@@ e . ( R ) 1 i@@ h@@ o@@ r@@ a . 3@@ 2 i@@ m@@ i@@ z@@ u@@ z@@ u . H@@ B@@ O n@@ g@@ o@@ L@@ w@@ e@@ s@@ i@@ h@@ l@@ a@@ n@@ u . 1 : 3@@ 5 a@@ .@@ m@@ .\n",
            "F@@ u@@ t@@ h@@ i l@@ o@@ w@@ o m@@ p@@ h@@ a@@ k@@ a@@ t@@ h@@ i m@@ u@@ k@@ h@@ u@@ l@@ u .\n",
            "U@@ -@@ H@@ o@@ w@@ a@@ r@@ d C@@ a@@ r@@ t@@ e@@ r w@@ e@@ n@@ q@@ a@@ b@@ a u@@ k@@ u@@ k@@ h@@ o@@ l@@ e@@ l@@ w@@ a u@@ k@@ u@@ t@@ h@@ i k@@ u@@ k@@ h@@ o@@ n@@ a i@@ s@@ i@@ q@@ a@@ l@@ e@@ k@@ i@@ s@@ o e@@ s@@ i@@ t@@ h@@ i@@ l@@ e f@@ u@@ t@@ h@@ i w@@ a@@ f@@ a n@@ g@@ e@@ n@@ d@@ l@@ e@@ l@@ a e@@ v@@ a@@ m@@ i@@ l@@ e n@@ g@@ o@@ -@@ 1@@ 9@@ 4@@ 9 .\n",
            "&@@ q@@ u@@ o@@ t@@ ; A@@ m@@ a@@ -@@ M@@ P n@@ a@@ w@@ o a@@ k@@ u@@ g@@ x@@ e@@ k@@ i@@ l@@ e u@@ k@@ u@@ t@@ h@@ u@@ n@@ y@@ e@@ l@@ w@@ a k@@ w@@ a@@ m@@ a@@ s@@ o@@ s@@ h@@ a a@@ s@@ e@@ M@@ e@@ l@@ i@@ k@@ a e@@ m@@ a@@ k@@ l@@ a@@ m@@ u a@@ s@@ e@@ P@@ o@@ l@@ a@@ n@@ d n@@ a@@ k@@ w@@ a@@ m@@ a@@ n@@ y@@ e a@@ m@@ a@@ z@@ w@@ e a@@ M@@ a@@ p@@ h@@ a@@ k@@ a@@ t@@ h@@ i n@@ a@@ s@@ e@@ M@@ p@@ u@@ m@@ a@@ l@@ a@@ n@@ g@@ a n@@ e@@ Y@@ u@@ r@@ o@@ p@@ h@@ u , e@@ t@@ h@@ i l@@ e@@ z@@ o z@@ e@@ n@@ z@@ o z@@ o@@ z@@ i@@ t@@ h@@ a@@ t@@ h@@ w@@ a n@@ g@@ o@@ k@@ u@@ t@@ h@@ i &@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ; z@@ i@@ w@@ u@@ k@@ u@@ l@@ u@@ n@@ g@@ i@@ s@@ e@@ l@@ e@@ l@@ a i@@ m@@ p@@ i &@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ; k@@ u@@ p@@ h@@ e@@ l@@ a . &@@ q@@ u@@ o@@ t@@ ;\n",
            "I@@ -@@ M@@ P y@@ e@@ -@@ C@@ o@@ n@@ s@@ e@@ r@@ v@@ a@@ t@@ i@@ v@@ e e@@ y@@ a@@ t@@ h@@ u@@ m@@ e@@ l@@ a i@@ n@@ q@@ w@@ a@@ b@@ a e@@ n@@ g@@ u@@ -@@ 2 , 0@@ 0@@ 0 â€œ y@@ e@@ m@@ i@@ l@@ a@@ y@@ e@@ z@@ o y@@ o@@ c@@ a@@ n@@ s@@ i â€ k@@ u@@ b@@ a@@ n@@ t@@ u b@@ e@@ s@@ i@@ f@@ a@@ z@@ a@@ n@@ e a@@ b@@ a@@ b@@ i@@ l@@ i e@@ z@@ i@@ n@@ s@@ u@@ k@@ w@@ i@@ n@@ i n@@ j@@ e e@@ z@@ i@@ n@@ g@@ u@@ -@@ 2@@ 1 k@@ u@@ t@@ h@@ i@@ w@@ e a@@ y@@ e@@ n@@ z@@ a@@ n@@ g@@ a o@@ k@@ u@@ b@@ i n@@ g@@ o@@ k@@ u@@ s@@ h@@ o k@@ w@@ a@@ b@@ a@@ q@@ a@@ p@@ h@@ i b@@ e@@ z@@ i@@ m@@ i@@ s@@ o y@@ a@@ s@@ e@@ p@@ h@@ a@@ l@@ a@@ m@@ e@@ n@@ d@@ e .\n",
            "U@@ -@@ D@@ u@@ s@@ t@@ i@@ n B@@ i@@ l@@ y@@ k , u@@ m@@ b@@ h@@ a@@ l@@ i w@@ a@@ s@@ e@@ -@@ C@@ a@@ n@@ a@@ d@@ a , w@@ a@@ c@@ h@@ a@@ z@@ a i@@ n@@ d@@ l@@ e@@ l@@ a &@@ a@@ p@@ o@@ s@@ ; a@@ y@@ e@@ s@@ e@@ y@@ i@@ n@@ d@@ o@@ d@@ a e@@ h@@ l@@ u@@ k@@ u@@ m@@ e@@ z@@ e@@ k@@ i@@ l@@ e n@@ g@@ a@@ y@@ o k@@ a@@ k@@ a@@ d@@ e &@@ a@@ p@@ o@@ s@@ ; n@@ g@@ e@@ s@@ i@@ k@@ h@@ a@@ t@@ h@@ i o@@ w@@ a@@ y@@ e@@ n@@ g@@ u@@ n@@ k@@ o@@ s@@ i@@ k@@ a@@ z@@ i w@@ a@@ k@@ h@@ e e@@ v@@ u@@ m@@ a , e@@ t@@ h@@ i â€˜ u@@ b@@ e@@ z@@ e@@ n@@ z@@ i@@ s@@ a n@@ g@@ o@@ k@@ u@@ b@@ a k@@ u@@ l@@ o m@@ s@@ h@@ a@@ d@@ o i@@ m@@ i@@ n@@ y@@ a@@ k@@ a e@@ m@@ i@@ b@@ i@@ l@@ i â€™ .\n",
            "I@@ -@@ H@@ u@@ a@@ w@@ e@@ i i@@ n@@ g@@ a@@ b@@ a y@@ i@@ n@@ k@@ a@@ m@@ p@@ a@@ n@@ i y@@ o@@ k@@ u@@ q@@ a@@ l@@ a e@@ z@@ o@@ l@@ i@@ m@@ a@@ l@@ a e@@ m@@ b@@ a@@ n@@ g@@ w@@ e@@ n@@ i w@@ e@@ C@@ h@@ i@@ n@@ a n@@ e@@ M@@ e@@ l@@ i@@ k@@ a .\n",
            "&@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ; K@@ u@@ y@@ a@@ n@@ g@@ i@@ j@@ a@@ b@@ u@@ l@@ i@@ s@@ a u@@ k@@ u@@ z@@ w@@ a u@@ m@@ b@@ o@@ n@@ o w@@ a@@ b@@ o , &@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ; k@@ u@@ p@@ h@@ e@@ n@@ d@@ u@@ l@@ a u@@ J@@ o@@ s@@ h@@ u@@ a , n@@ g@@ a@@ p@@ h@@ a@@ m@@ b@@ i k@@ o@@ k@@ u@@ g@@ c@@ i@@ z@@ e@@ l@@ e@@ l@@ a u@@ k@@ u@@ t@@ h@@ i a@@ k@@ a@@ s@@ i@@ l@@ o i@@ q@@ h@@ a@@ w@@ e e@@ l@@ i@@ n@@ g@@ a@@ q@@ e@@ d@@ a i@@ z@@ i@@ n@@ k@@ i@@ n@@ g@@ a z@@ o@@ m@@ h@@ l@@ a@@ b@@ a n@@ g@@ o@@ m@@ l@@ i@@ n@@ g@@ o . U@@ m@@ a e@@ b@@ h@@ e@@ k@@ e@@ n@@ e n@@ e@@ n@@ c@@ i@@ n@@ d@@ e@@ z@@ i , u@@ y@@ a@@ n@@ g@@ u@@ n@@ u@@ z@@ a e@@ s@@ h@@ o i@@ n@@ t@@ o e@@ t@@ h@@ i@@ l@@ e &@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ; n@@ g@@ o@@ k@@ u@@ v@@ u@@ s@@ e@@ l@@ e@@ l@@ a &@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ; b@@ e@@ s@@ e a@@ p@@ h@@ a@@ k@@ a@@ m@@ i@@ s@@ e u@@ k@@ u@@ t@@ h@@ i a@@ n@@ g@@ a@@ s@@ e@@ b@@ e@@ n@@ z@@ i@@ s@@ a u@@ b@@ u@@ d@@ l@@ e@@ l@@ w@@ a@@ n@@ o b@@ a@@ k@@ h@@ e n@@ e@@ -@@ S@@ a@@ u@@ d@@ i@@ s u@@ k@@ u@@ b@@ u@@ z@@ a i@@ m@@ i@@ b@@ u@@ z@@ o e@@ n@@ z@@ i@@ m@@ a k@@ u@@ n@@ o@@ k@@ u@@ b@@ a &@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ; a@@ s@@ o@@ l@@ e , a@@ k@@ h@@ o@@ m@@ b@@ e n@@ g@@ e@@ m@@ i@@ n@@ w@@ e f@@ u@@ t@@ h@@ i a@@ m@@ e@@ m@@ e@@ z@@ e e@@ s@@ e@@ G@@ r@@ e@@ a@@ t B@@ r@@ i@@ t@@ a@@ i@@ n . &@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ; &@@ q@@ u@@ o@@ t@@ ;\n",
            "\n",
            "==> dev.en <==\n",
            "Conor Garland added his team-leading 11th goal in the third period, and Clayton Keller notched two assists for the Coyotes, who leapfrogged idle Edmonton into first place in the Pacific Division. Arizona has won the first two contests of its four-game road trip and improved to 10-3-3 away from home this season.\n",
            "One shows a girl next to her 20-year-old brother in military uniform.\n",
            "Yo Soy Muy Macho (1953) Silvia Pinal, Miguel Torruco.\n",
            "On the other side, President Vladimir V. Putin of Russia, having promoted himself for years as a defender of Russia's reach as an Orthodox power, also has a keen political interest and desperately wants to keep Ukraine under the wing of the Moscow patriarch.\n",
            "Returning to the present, our smoothie cups lie empty at Steve's feet, and he rubs his stomach cartoonishly while he yawns.\n",
            "\"The Conservatives have promised an end to \"\"the complicated franchising model\"\" devised by John Major's government in the 1990s, but have recently awarded a new franchise on the West Coast main line to a consortium involving FirstGroup and the Italian state railway.\"\n",
            "Saint-Paul-les-Durance, France:\n",
            "The trial of HernÃ¡ndez, 21, is set to begin Monday in what looks to be the first test for women's reproductive rights under Bukele, who is young and has expressed disdain for all forms of discrimination.\n",
            "BMW worker Mark, 51, was diagnosed with stage four terminal cancer last month\n",
            "As the plates of the planet's crust smashed into each other, they left vast areas of oceanic rock exposed.\n",
            "\n",
            "==> dev.zu <==\n",
            "Ngibuke umdlalo, ngibe nomzuzu wenjabulo ephelele.\n",
            "Bachaza ukuthi umoya esiwuphefumulayo uwagcwalisa kanjani amaphaphu ethu nge-oksijini.\n",
            "UConor Garland ufake igoli le-11 leqembu lakhe ebelihamba phambili ehlandleni lesithathu, bese uClayton Keller wazisa kabili kumaCoyote, ogxumise i-Edmonton yaba sendaweni yokuqala Ophikweni lakuPacific. I-Arizona inqobe imincintiswano emibili yokuqala ohambweni lwayo lwemidlalo emine futhi yathuthukela ku-10-3-3 engakho ekhaya kulesi sikhathi sokudlala.\n",
            "Umuntu uveza intombazane eseduze kukamfowabo oneminyaka engama-20 ogqoke umfaniswano wamasosha.\n",
            "Yo Soy Muy Macho (1953) Silvia Pinal, Miguel Torruco.\n",
            "Kwelinye icala, uMongameli Vadimir V. Putin waseRussia, oseneminyaka azibeka njengomvikeli weRussia ukuze ibe namandla kwezeNkolo, uphinde abe nentshisekelo kwezepolitiki futhi ufuna ngamandla akhe onke ukugcina i-Ukraine ingaphansi kwamandla eMoscow.\n",
            "Uma kubuyelwa kokwamanje, izinkomishi zethu ze-smoothie zisezinyaweni zikaSteve zingenalutho phakathi, futhi uhlikihla isisu sakhe njengopopayi ebe ezamula.\n",
            "\"Ama-Conservative athembise ukuphela \"\"kohlelo lwelungelo lokuvota oluxakayo\"\" olwakhiwa uhulumeni kaJohn major ngeminyaka yo-1990, kodwa asanda kuhlinzeka ngohlelo lwelungelo lokuvota elisha kuWest Coast main line enhlanganweni ebandakanya i-FirstGroup kanye ne-Italian state railway.\"\n",
            "Saint-Paul-les-Durance, France:\n",
            "Icala likaHernÃ¡ndez, 21, kuhlelwe ukuba liqale ngoMsombuluko kulokho okubukeka kungukuhlolwa kokuqala kwamalungelo ezokuthola abantwana kwabantu besifazane ngaphansi kukaBukele, osemncane futhi oveze ukucasuka kwakhe ngazo zonke izinhlobo zokucwaswa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBRMm4kMxZ8L",
        "outputId": "693d26e7-7bba-40a5-838f-720f9ed39688"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install .\n",
        "# Install Pytorch with GPU support v1.7.1.\n",
        "# ! pip install torch==1.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'joeynmt' already exists and is not an empty directory.\n",
            "Processing /content/joeynmt\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.19.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.4.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.10.0+cu111)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.7.0)\n",
            "Requirement already satisfied: torchtext>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.0)\n",
            "Requirement already satisfied: sacrebleu>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.0.0)\n",
            "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.3.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (6.0)\n",
            "Requirement already satisfied: pylint>=2.9.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.11.1)\n",
            "Requirement already satisfied: six==1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.12.0)\n",
            "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (3.10.0.2)\n",
            "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (0.6.1)\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (0.10.2)\n",
            "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (5.10.1)\n",
            "Requirement already satisfied: astroid<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (2.8.5)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (2.4.0)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.9,>=2.8.0->pylint>=2.9.6->joeynmt==1.3) (1.6.0)\n",
            "Requirement already satisfied: typed-ast<2.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.9,>=2.8.0->pylint>=2.9.6->joeynmt==1.3) (1.5.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.3) (0.8.9)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.3) (0.4.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.3) (2.3.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.3) (2019.12.20)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.42.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.37.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext>=0.10.0->joeynmt==1.3) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (3.0.6)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
            "Building wheels for collected packages: joeynmt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=86029 sha256=5f4a3b906fc262e2bd76922cf1f7d8f85074c3ba573c02411e7d51e4fc518af2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9m1ym5vw/wheels/0a/f4/bf/6c9d3b8efbfece6cd209f865be37382b02e7c3584df2e28ca4\n",
            "Successfully built joeynmt\n",
            "Installing collected packages: joeynmt\n",
            "  Attempting uninstall: joeynmt\n",
            "    Found existing installation: joeynmt 1.3\n",
            "    Uninstalling joeynmt-1.3:\n",
            "      Successfully uninstalled joeynmt-1.3\n",
            "Successfully installed joeynmt-1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-TyjtmXB1mL",
        "outputId": "1cffc8b2-0c05-4f71-ac8b-e2f5a61ca1b5"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p \"$data_path\"\n",
        "! cp train.* \"$data_path\"\n",
        "! cp test.* \"$data_path\"\n",
        "! cp dev.* \"$data_path\"\n",
        "! cp bpe.codes.4000 \"$data_path\"\n",
        "! ls \"$data_path\"\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Test language Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bpe.codes.4000\tdev.zu\t     test.en-any.en    train.bpe.en  vocab.txt\n",
            "dev.bpe.en\ttest.bpe.en  test.en-any.en.1  train.bpe.zu\n",
            "dev.bpe.zu\ttest.bpe.zu  test.en-any.en.2  train.en\n",
            "dev.en\t\ttest.en      test.zu\t       train.zu\n",
            "bpe.codes.4000\tdev.zu\t     test.en-any.en    train.bpe.en\n",
            "dev.bpe.en\ttest.bpe.en  test.en-any.en.1  train.bpe.zu\n",
            "dev.bpe.zu\ttest.bpe.zu  test.en-any.en.2  train.en\n",
            "dev.en\t\ttest.en      test.zu\t       train.zu\n",
            "BPE Test language Sentences\n",
            "Ng@@ en@@ xa yal@@ okho , ngang@@ aziwa njengom@@ untu ong@@ ath@@ emb@@ ekile .\n",
            "L@@ apho ng@@ ifund@@ a iq@@ in@@ iso , ngen@@ qaba ukuqhubeka nal@@ ow@@ o m@@ kh@@ uba , n@@ aku@@ ba lo m@@ sebenzi waw@@ ung@@ i@@ hol@@ ela kahle kakhulu .\n",
            "Ng@@ iy@@ is@@ ibon@@ elo es@@ ihl@@ e em@@ ad@@ od@@ an@@ eni ami amab@@ ili futhi seng@@ i@@ ye ng@@ af@@ anel@@ ekela amal@@ ung@@ elo eb@@ andl@@ eni .\n",
            "Kub@@ ac@@ wan@@ ingi - m@@ abh@@ uku ent@@ ela nabanye eng@@ isebenz@@ elana nabo ebh@@ izin@@ is@@ ini , manje seng@@ aziwa njengom@@ untu oth@@ emb@@ ekile . â€\n",
            "U@@ R@@ uthe wath@@ uth@@ ela kw@@ a - Is@@ r@@ ay@@ eli lapho ay@@ ey@@ ok@@ wazi khona uku@@ khul@@ ekela uN@@ kul@@ un@@ kulu we@@ q@@ in@@ iso .\n",
            "Combined BPE Vocab\n",
            "Z\n",
            "+\n",
            "dit@@\n",
            "ram\n",
            "Sec@@\n",
            "ishiy@@\n",
            "stem@@\n",
            "beca@@\n",
            "suc@@\n",
            "aweni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlMitUHR8Qy-",
        "outputId": "d4cbe487-d474-488f-e886-391fd2f77354"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bpe.codes.4000\tdev.zu\t     test.en-any.en    train.bpe.en\n",
            "dev.bpe.en\ttest.bpe.en  test.en-any.en.1  train.bpe.zu\n",
            "dev.bpe.zu\ttest.bpe.zu  test.en-any.en.2  train.en\n",
            "dev.en\t\ttest.en      test.zu\t       train.zu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIs1lY2hxMsl"
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "    sacrebleu:                      # sacrebleu options\n",
        "        remove_whitespace: True     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)\n",
        "        tokenize: \"none\"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: \"none\" (use for already tokenized test data), \"13a\" (default minimal tokenizer), \"intl\" which mostly does punctuation and unicode, etc) \n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 25\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 100          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 1\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZBPFwT94WpI",
        "outputId": "a78c129e-dc86-46bf-a32f-615a4511d9c3"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-20 17:29:03,215 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-11-20 17:29:03,248 - INFO - joeynmt.data - Loading training data...\n",
            "2021-11-20 17:29:03,263 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-11-20 17:29:03,526 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-11-20 17:29:03,529 - INFO - joeynmt.data - Loading test data...\n",
            "2021-11-20 17:29:03,561 - INFO - joeynmt.data - Data loaded.\n",
            "2021-11-20 17:29:03,562 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-11-20 17:29:03,875 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-11-20 17:29:06,883 - INFO - joeynmt.training - Total params: 12112128\n",
            "2021-11-20 17:29:06,886 - WARNING - joeynmt.training - `keep_last_ckpts` option is outdated. Please use `keep_best_ckpts`, instead.\n",
            "2021-11-20 17:29:09,855 - INFO - joeynmt.helpers - cfg.name                           : enzu_transformer\n",
            "2021-11-20 17:29:09,855 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-11-20 17:29:09,855 - INFO - joeynmt.helpers - cfg.data.trg                       : zu\n",
            "2021-11-20 17:29:09,856 - INFO - joeynmt.helpers - cfg.data.train                     : data/enzu/train.bpe\n",
            "2021-11-20 17:29:09,856 - INFO - joeynmt.helpers - cfg.data.dev                       : data/enzu/dev.bpe\n",
            "2021-11-20 17:29:09,856 - INFO - joeynmt.helpers - cfg.data.test                      : data/enzu/test.bpe\n",
            "2021-11-20 17:29:09,856 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-11-20 17:29:09,856 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-11-20 17:29:09,856 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-11-20 17:29:09,856 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/enzu/vocab.txt\n",
            "2021-11-20 17:29:09,857 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/enzu/vocab.txt\n",
            "2021-11-20 17:29:09,857 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-11-20 17:29:09,857 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-11-20 17:29:09,857 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.remove_whitespace : True\n",
            "2021-11-20 17:29:09,857 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.tokenize     : none\n",
            "2021-11-20 17:29:09,857 - INFO - joeynmt.helpers - cfg.training.random_seed           : 25\n",
            "2021-11-20 17:29:09,857 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-11-20 17:29:09,858 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-11-20 17:29:09,858 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-11-20 17:29:09,858 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-11-20 17:29:09,858 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-11-20 17:29:09,858 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-11-20 17:29:09,858 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-11-20 17:29:09,858 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-11-20 17:29:09,858 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-11-20 17:29:09,859 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-11-20 17:29:09,859 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-11-20 17:29:09,859 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-11-20 17:29:09,859 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-11-20 17:29:09,859 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-11-20 17:29:09,859 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-11-20 17:29:09,859 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-11-20 17:29:09,859 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-11-20 17:29:09,860 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-11-20 17:29:09,860 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-11-20 17:29:09,860 - INFO - joeynmt.helpers - cfg.training.epochs                : 30\n",
            "2021-11-20 17:29:09,860 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 100\n",
            "2021-11-20 17:29:09,860 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-11-20 17:29:09,860 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-11-20 17:29:09,860 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/enzu_transformer\n",
            "2021-11-20 17:29:09,860 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-11-20 17:29:09,861 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-11-20 17:29:09,861 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-11-20 17:29:09,861 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-11-20 17:29:09,861 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-11-20 17:29:09,861 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 1\n",
            "2021-11-20 17:29:09,861 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-11-20 17:29:09,861 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-11-20 17:29:09,862 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-11-20 17:29:09,862 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-11-20 17:29:09,862 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-11-20 17:29:09,862 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-11-20 17:29:09,862 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-11-20 17:29:09,862 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-11-20 17:29:09,862 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-11-20 17:29:09,862 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-11-20 17:29:09,863 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-11-20 17:29:09,863 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-11-20 17:29:09,863 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-11-20 17:29:09,863 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-11-20 17:29:09,863 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-11-20 17:29:09,863 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-11-20 17:29:09,863 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-11-20 17:29:09,864 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-11-20 17:29:09,864 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-11-20 17:29:09,864 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-11-20 17:29:09,864 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-11-20 17:29:09,864 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-11-20 17:29:09,864 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-11-20 17:29:09,864 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-11-20 17:29:09,865 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-11-20 17:29:09,865 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 828,\n",
            "\tvalid 100,\n",
            "\ttest 2711\n",
            "2021-11-20 17:29:09,865 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] Pet@@ er V@@ an Sant@@ : And it means wh@@ at@@ ?\n",
            "\t[TRG] Pet@@ er V@@ an Sant@@ : B@@ ese kusho ukuth@@ in@@ i?\n",
            "2021-11-20 17:29:09,865 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) a (6) to (7) and (8) of (9) in\n",
            "2021-11-20 17:29:09,865 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) a (6) to (7) and (8) of (9) in\n",
            "2021-11-20 17:29:09,865 - INFO - joeynmt.helpers - Number of Src words (types): 4109\n",
            "2021-11-20 17:29:09,866 - INFO - joeynmt.helpers - Number of Trg words (types): 4109\n",
            "2021-11-20 17:29:09,866 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4109),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4109))\n",
            "2021-11-20 17:29:09,870 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-11-20 17:29:09,870 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-11-20 17:29:17,212 - INFO - joeynmt.training - Epoch   1: total training loss 129.80\n",
            "2021-11-20 17:29:17,213 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-11-20 17:29:24,442 - INFO - joeynmt.training - Epoch   2: total training loss 120.86\n",
            "2021-11-20 17:29:24,443 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-11-20 17:29:31,677 - INFO - joeynmt.training - Epoch   3: total training loss 119.21\n",
            "2021-11-20 17:29:31,677 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-11-20 17:29:38,853 - INFO - joeynmt.training - Epoch   4: total training loss 119.11\n",
            "2021-11-20 17:29:38,854 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-11-20 17:29:46,049 - INFO - joeynmt.training - Epoch   5: total training loss 118.92\n",
            "2021-11-20 17:29:46,050 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-11-20 17:29:47,869 - INFO - joeynmt.training - Epoch   6, Step:      100, Batch Loss:     6.236739, Tokens per Sec:     4542, Lr: 0.000300\n",
            "2021-11-20 17:29:48,900 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-11-20 17:29:49,353 - INFO - joeynmt.training - Example #0\n",
            "2021-11-20 17:29:49,354 - INFO - joeynmt.training - \tSource:     Conor Garland added his team-leading 11th goal in the third period, and Clayton Keller notched two assists for the Coyotes, who leapfrogged idle Edmonton into first place in the Pacific Division. Arizona has won the first two contests of its four-game road trip and improved to 10-3-3 away from home this season.\n",
            "2021-11-20 17:29:49,354 - INFO - joeynmt.training - \tReference:  Ngibuke umdlalo, ngibe nomzuzu wenjabulo ephelele.\n",
            "2021-11-20 17:29:49,354 - INFO - joeynmt.training - \tHypothesis: UUUukuthi ukuthi ukuthi ukuthi ukuthi\n",
            "2021-11-20 17:29:49,354 - INFO - joeynmt.training - Example #1\n",
            "2021-11-20 17:29:49,355 - INFO - joeynmt.training - \tSource:     One shows a girl next to her 20-year-old brother in military uniform.\n",
            "2021-11-20 17:29:49,362 - INFO - joeynmt.training - \tReference:  Bachaza ukuthi umoya esiwuphefumulayo uwagcwalisa kanjani amaphaphu ethu nge-oksijini.\n",
            "2021-11-20 17:29:49,362 - INFO - joeynmt.training - \tHypothesis: UUUukuthi ukuthi ukuthi ukuthi\n",
            "2021-11-20 17:29:49,362 - INFO - joeynmt.training - Example #2\n",
            "2021-11-20 17:29:49,363 - INFO - joeynmt.training - \tSource:     Yo Soy Muy Macho (1953) Silvia Pinal, Miguel Torruco.\n",
            "2021-11-20 17:29:49,363 - INFO - joeynmt.training - \tReference:  UConor Garland ufake igoli le-11 leqembu lakhe ebelihamba phambili ehlandleni lesithathu, bese uClayton Keller wazisa kabili kumaCoyote, ogxumise i-Edmonton yaba sendaweni yokuqala Ophikweni lakuPacific. I-Arizona inqobe imincintiswano emibili yokuqala ohambweni lwayo lwemidlalo emine futhi yathuthukela ku-10-3-3 engakho ekhaya kulesi sikhathi sokudlala.\n",
            "2021-11-20 17:29:49,363 - INFO - joeynmt.training - \tHypothesis: UUUukuthi ukuthi ukuthi ukuthi\n",
            "2021-11-20 17:29:49,363 - INFO - joeynmt.training - Example #3\n",
            "2021-11-20 17:29:49,364 - INFO - joeynmt.training - \tSource:     On the other side, President Vladimir V. Putin of Russia, having promoted himself for years as a defender of Russia's reach as an Orthodox power, also has a keen political interest and desperately wants to keep Ukraine under the wing of the Moscow patriarch.\n",
            "2021-11-20 17:29:49,364 - INFO - joeynmt.training - \tReference:  Umuntu uveza intombazane eseduze kukamfowabo oneminyaka engama-20 ogqoke umfaniswano wamasosha.\n",
            "2021-11-20 17:29:49,364 - INFO - joeynmt.training - \tHypothesis: UUUukuthi ukuthi ukuthi ukuthi ukuthi\n",
            "2021-11-20 17:29:49,366 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step      100: bleu:   0.02, loss: 29641.0430, ppl: 469.1253, duration: 1.4963s\n",
            "2021-11-20 17:29:54,744 - INFO - joeynmt.training - Epoch   6: total training loss 118.65\n",
            "2021-11-20 17:29:54,745 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-11-20 17:30:01,998 - INFO - joeynmt.training - Epoch   7: total training loss 118.18\n",
            "2021-11-20 17:30:01,999 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-11-20 17:30:09,204 - INFO - joeynmt.training - Epoch   8: total training loss 117.68\n",
            "2021-11-20 17:30:09,204 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-11-20 17:30:16,433 - INFO - joeynmt.training - Epoch   9: total training loss 117.21\n",
            "2021-11-20 17:30:16,434 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-11-20 17:30:23,644 - INFO - joeynmt.training - Epoch  10: total training loss 116.70\n",
            "2021-11-20 17:30:23,645 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-11-20 17:30:27,444 - INFO - joeynmt.training - Epoch  11, Step:      200, Batch Loss:     6.083000, Tokens per Sec:     4747, Lr: 0.000300\n",
            "2021-11-20 17:30:40,353 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-11-20 17:30:40,871 - INFO - joeynmt.helpers - delete models/enzu_transformer/100.ckpt\n",
            "2021-11-20 17:30:40,893 - INFO - joeynmt.helpers - delete /content/joeynmt/models/enzu_transformer/100.ckpt\n",
            "2021-11-20 17:30:40,893 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/joeynmt/models/enzu_transformer/100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/joeynmt/models/enzu_transformer/100.ckpt')\n",
            "2021-11-20 17:30:40,894 - INFO - joeynmt.training - Example #0\n",
            "2021-11-20 17:30:40,894 - INFO - joeynmt.training - \tSource:     Conor Garland added his team-leading 11th goal in the third period, and Clayton Keller notched two assists for the Coyotes, who leapfrogged idle Edmonton into first place in the Pacific Division. Arizona has won the first two contests of its four-game road trip and improved to 10-3-3 away from home this season.\n",
            "2021-11-20 17:30:40,894 - INFO - joeynmt.training - \tReference:  Ngibuke umdlalo, ngibe nomzuzu wenjabulo ephelele.\n",
            "2021-11-20 17:30:40,895 - INFO - joeynmt.training - \tHypothesis: \"Ulllllllllllllllllllzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
            "2021-11-20 17:30:40,895 - INFO - joeynmt.training - Example #1\n",
            "2021-11-20 17:30:40,895 - INFO - joeynmt.training - \tSource:     One shows a girl next to her 20-year-old brother in military uniform.\n",
            "2021-11-20 17:30:40,895 - INFO - joeynmt.training - \tReference:  Bachaza ukuthi umoya esiwuphefumulayo uwagcwalisa kanjani amaphaphu ethu nge-oksijini.\n",
            "2021-11-20 17:30:40,895 - INFO - joeynmt.training - \tHypothesis: \"\"\"UKKwwi-i-i-i-i-i-i-i-a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "2021-11-20 17:30:40,895 - INFO - joeynmt.training - Example #2\n",
            "2021-11-20 17:30:40,896 - INFO - joeynmt.training - \tSource:     Yo Soy Muy Macho (1953) Silvia Pinal, Miguel Torruco.\n",
            "2021-11-20 17:30:40,896 - INFO - joeynmt.training - \tReference:  UConor Garland ufake igoli le-11 leqembu lakhe ebelihamba phambili ehlandleni lesithathu, bese uClayton Keller wazisa kabili kumaCoyote, ogxumise i-Edmonton yaba sendaweni yokuqala Ophikweni lakuPacific. I-Arizona inqobe imincintiswano emibili yokuqala ohambweni lwayo lwemidlalo emine futhi yathuthukela ku-10-3-3 engakho ekhaya kulesi sikhathi sokudlala.\n",
            "2021-11-20 17:30:40,896 - INFO - joeynmt.training - \tHypothesis: \"\"\"UKKi-i-i-i-i-i-i-i-i-a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n",
            "2021-11-20 17:30:40,896 - INFO - joeynmt.training - Example #3\n",
            "2021-11-20 17:30:40,896 - INFO - joeynmt.training - \tSource:     On the other side, President Vladimir V. Putin of Russia, having promoted himself for years as a defender of Russia's reach as an Orthodox power, also has a keen political interest and desperately wants to keep Ukraine under the wing of the Moscow patriarch.\n",
            "2021-11-20 17:30:40,897 - INFO - joeynmt.training - \tReference:  Umuntu uveza intombazane eseduze kukamfowabo oneminyaka engama-20 ogqoke umfaniswano wamasosha.\n",
            "2021-11-20 17:30:40,897 - INFO - joeynmt.training - \tHypothesis: \"Ullllllllllllllllllllzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
            "2021-11-20 17:30:40,897 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step      200: bleu:   0.00, loss: 29303.0625, ppl: 437.3504, duration: 13.4529s\n",
            "2021-11-20 17:30:44,280 - INFO - joeynmt.training - Epoch  11: total training loss 116.02\n",
            "2021-11-20 17:30:44,281 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-11-20 17:30:51,610 - INFO - joeynmt.training - Epoch  12: total training loss 121.45\n",
            "2021-11-20 17:30:51,610 - INFO - joeynmt.training - EPOCH 13\n",
            "2021-11-20 17:30:58,765 - INFO - joeynmt.training - Epoch  13: total training loss 114.99\n",
            "2021-11-20 17:30:58,765 - INFO - joeynmt.training - EPOCH 14\n",
            "2021-11-20 17:31:06,060 - INFO - joeynmt.training - Epoch  14: total training loss 120.46\n",
            "2021-11-20 17:31:06,061 - INFO - joeynmt.training - EPOCH 15\n",
            "2021-11-20 17:31:13,290 - INFO - joeynmt.training - Epoch  15: total training loss 120.09\n",
            "2021-11-20 17:31:13,291 - INFO - joeynmt.training - EPOCH 16\n",
            "2021-11-20 17:31:17,704 - INFO - joeynmt.training - Epoch  16, Step:      300, Batch Loss:     6.008382, Tokens per Sec:     4596, Lr: 0.000300\n",
            "2021-11-20 17:31:30,616 - INFO - joeynmt.training - Example #0\n",
            "2021-11-20 17:31:30,616 - INFO - joeynmt.training - \tSource:     Conor Garland added his team-leading 11th goal in the third period, and Clayton Keller notched two assists for the Coyotes, who leapfrogged idle Edmonton into first place in the Pacific Division. Arizona has won the first two contests of its four-game road trip and improved to 10-3-3 away from home this season.\n",
            "2021-11-20 17:31:30,616 - INFO - joeynmt.training - \tReference:  Ngibuke umdlalo, ngibe nomzuzu wenjabulo ephelele.\n",
            "2021-11-20 17:31:30,617 - INFO - joeynmt.training - \tHypothesis: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUBBBBBBBBBBBBBBBBBBBBBBBB\n",
            "2021-11-20 17:31:30,617 - INFO - joeynmt.training - Example #1\n",
            "2021-11-20 17:31:30,617 - INFO - joeynmt.training - \tSource:     One shows a girl next to her 20-year-old brother in military uniform.\n",
            "2021-11-20 17:31:30,617 - INFO - joeynmt.training - \tReference:  Bachaza ukuthi umoya esiwuphefumulayo uwagcwalisa kanjani amaphaphu ethu nge-oksijini.\n",
            "2021-11-20 17:31:30,617 - INFO - joeynmt.training - \tHypothesis: \"\"\"\"\"\"UBBBBBBBBBBBwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwi i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i\n",
            "2021-11-20 17:31:30,617 - INFO - joeynmt.training - Example #2\n",
            "2021-11-20 17:31:30,618 - INFO - joeynmt.training - \tSource:     Yo Soy Muy Macho (1953) Silvia Pinal, Miguel Torruco.\n",
            "2021-11-20 17:31:30,618 - INFO - joeynmt.training - \tReference:  UConor Garland ufake igoli le-11 leqembu lakhe ebelihamba phambili ehlandleni lesithathu, bese uClayton Keller wazisa kabili kumaCoyote, ogxumise i-Edmonton yaba sendaweni yokuqala Ophikweni lakuPacific. I-Arizona inqobe imincintiswano emibili yokuqala ohambweni lwayo lwemidlalo emine futhi yathuthukela ku-10-3-3 engakho ekhaya kulesi sikhathi sokudlala.\n",
            "2021-11-20 17:31:30,618 - INFO - joeynmt.training - \tHypothesis: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"UKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKKK\n",
            "2021-11-20 17:31:30,618 - INFO - joeynmt.training - Example #3\n",
            "2021-11-20 17:31:30,618 - INFO - joeynmt.training - \tSource:     On the other side, President Vladimir V. Putin of Russia, having promoted himself for years as a defender of Russia's reach as an Orthodox power, also has a keen political interest and desperately wants to keep Ukraine under the wing of the Moscow patriarch.\n",
            "2021-11-20 17:31:30,619 - INFO - joeynmt.training - \tReference:  Umuntu uveza intombazane eseduze kukamfowabo oneminyaka engama-20 ogqoke umfaniswano wamasosha.\n",
            "2021-11-20 17:31:30,619 - INFO - joeynmt.training - \tHypothesis: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUBBBBBBBBBBBBBBBBBBBBBBBB\n",
            "2021-11-20 17:31:30,619 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step      300: bleu:   0.01, loss: 29319.3848, ppl: 438.8344, duration: 12.9146s\n",
            "2021-11-20 17:31:33,455 - INFO - joeynmt.training - Epoch  16: total training loss 119.88\n",
            "2021-11-20 17:31:33,456 - INFO - joeynmt.training - EPOCH 17\n",
            "2021-11-20 17:31:40,656 - INFO - joeynmt.training - Epoch  17: total training loss 119.64\n",
            "2021-11-20 17:31:40,657 - INFO - joeynmt.training - EPOCH 18\n",
            "2021-11-20 17:31:47,944 - INFO - joeynmt.training - Epoch  18: total training loss 119.10\n",
            "2021-11-20 17:31:47,944 - INFO - joeynmt.training - EPOCH 19\n",
            "2021-11-20 17:31:55,224 - INFO - joeynmt.training - Epoch  19: total training loss 118.59\n",
            "2021-11-20 17:31:55,225 - INFO - joeynmt.training - EPOCH 20\n",
            "2021-11-20 17:32:02,366 - INFO - joeynmt.training - Epoch  20: total training loss 112.08\n",
            "2021-11-20 17:32:02,366 - INFO - joeynmt.training - EPOCH 21\n",
            "2021-11-20 17:32:07,317 - INFO - joeynmt.training - Epoch  21, Step:      400, Batch Loss:     5.840568, Tokens per Sec:     4729, Lr: 0.000300\n",
            "2021-11-20 17:32:20,169 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-11-20 17:32:20,666 - INFO - joeynmt.helpers - delete models/enzu_transformer/200.ckpt\n",
            "2021-11-20 17:32:20,706 - INFO - joeynmt.helpers - delete /content/joeynmt/models/enzu_transformer/200.ckpt\n",
            "2021-11-20 17:32:20,706 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/joeynmt/models/enzu_transformer/200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/joeynmt/models/enzu_transformer/200.ckpt')\n",
            "2021-11-20 17:32:20,707 - INFO - joeynmt.training - Example #0\n",
            "2021-11-20 17:32:20,707 - INFO - joeynmt.training - \tSource:     Conor Garland added his team-leading 11th goal in the third period, and Clayton Keller notched two assists for the Coyotes, who leapfrogged idle Edmonton into first place in the Pacific Division. Arizona has won the first two contests of its four-game road trip and improved to 10-3-3 away from home this season.\n",
            "2021-11-20 17:32:20,707 - INFO - joeynmt.training - \tReference:  Ngibuke umdlalo, ngibe nomzuzu wenjabulo ephelele.\n",
            "2021-11-20 17:32:20,707 - INFO - joeynmt.training - \tHypothesis: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"UUUUUUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
            "2021-11-20 17:32:20,707 - INFO - joeynmt.training - Example #1\n",
            "2021-11-20 17:32:20,708 - INFO - joeynmt.training - \tSource:     One shows a girl next to her 20-year-old brother in military uniform.\n",
            "2021-11-20 17:32:20,708 - INFO - joeynmt.training - \tReference:  Bachaza ukuthi umoya esiwuphefumulayo uwagcwalisa kanjani amaphaphu ethu nge-oksijini.\n",
            "2021-11-20 17:32:20,708 - INFO - joeynmt.training - \tHypothesis: Uwwwbbbbbbbbsssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssnnnnnnnnnnnnnne.\n",
            "2021-11-20 17:32:20,708 - INFO - joeynmt.training - Example #2\n",
            "2021-11-20 17:32:20,708 - INFO - joeynmt.training - \tSource:     Yo Soy Muy Macho (1953) Silvia Pinal, Miguel Torruco.\n",
            "2021-11-20 17:32:20,708 - INFO - joeynmt.training - \tReference:  UConor Garland ufake igoli le-11 leqembu lakhe ebelihamba phambili ehlandleni lesithathu, bese uClayton Keller wazisa kabili kumaCoyote, ogxumise i-Edmonton yaba sendaweni yokuqala Ophikweni lakuPacific. I-Arizona inqobe imincintiswano emibili yokuqala ohambweni lwayo lwemidlalo emine futhi yathuthukela ku-10-3-3 engakho ekhaya kulesi sikhathi sokudlala.\n",
            "2021-11-20 17:32:20,709 - INFO - joeynmt.training - \tHypothesis: I-KBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBo o o o o o o o o o o o o o o o o o o o o o o e\n",
            "2021-11-20 17:32:20,709 - INFO - joeynmt.training - Example #3\n",
            "2021-11-20 17:32:20,709 - INFO - joeynmt.training - \tSource:     On the other side, President Vladimir V. Putin of Russia, having promoted himself for years as a defender of Russia's reach as an Orthodox power, also has a keen political interest and desperately wants to keep Ukraine under the wing of the Moscow patriarch.\n",
            "2021-11-20 17:32:20,709 - INFO - joeynmt.training - \tReference:  Umuntu uveza intombazane eseduze kukamfowabo oneminyaka engama-20 ogqoke umfaniswano wamasosha.\n",
            "2021-11-20 17:32:20,709 - INFO - joeynmt.training - \tHypothesis: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"UUUUUUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
            "2021-11-20 17:32:20,709 - INFO - joeynmt.training - Validation result (greedy) at epoch  21, step      400: bleu:   0.03, loss: 29250.2871, ppl: 432.5869, duration: 13.3922s\n",
            "2021-11-20 17:32:22,976 - INFO - joeynmt.training - Epoch  21: total training loss 111.37\n",
            "2021-11-20 17:32:22,976 - INFO - joeynmt.training - EPOCH 22\n",
            "2021-11-20 17:32:30,299 - INFO - joeynmt.training - Epoch  22: total training loss 116.59\n",
            "2021-11-20 17:32:30,299 - INFO - joeynmt.training - EPOCH 23\n",
            "2021-11-20 17:32:37,552 - INFO - joeynmt.training - Epoch  23: total training loss 115.78\n",
            "2021-11-20 17:32:37,553 - INFO - joeynmt.training - EPOCH 24\n",
            "2021-11-20 17:32:44,748 - INFO - joeynmt.training - Epoch  24: total training loss 115.06\n",
            "2021-11-20 17:32:44,749 - INFO - joeynmt.training - EPOCH 25\n",
            "2021-11-20 17:32:51,891 - INFO - joeynmt.training - Epoch  25: total training loss 108.39\n",
            "2021-11-20 17:32:51,892 - INFO - joeynmt.training - EPOCH 26\n",
            "2021-11-20 17:32:57,577 - INFO - joeynmt.training - Epoch  26, Step:      500, Batch Loss:     5.671086, Tokens per Sec:     4616, Lr: 0.000300\n",
            "2021-11-20 17:33:10,458 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-11-20 17:33:10,961 - INFO - joeynmt.helpers - delete models/enzu_transformer/400.ckpt\n",
            "2021-11-20 17:33:10,984 - INFO - joeynmt.helpers - delete /content/joeynmt/models/enzu_transformer/400.ckpt\n",
            "2021-11-20 17:33:10,984 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /content/joeynmt/models/enzu_transformer/400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/content/joeynmt/models/enzu_transformer/400.ckpt')\n",
            "2021-11-20 17:33:10,985 - INFO - joeynmt.training - Example #0\n",
            "2021-11-20 17:33:10,985 - INFO - joeynmt.training - \tSource:     Conor Garland added his team-leading 11th goal in the third period, and Clayton Keller notched two assists for the Coyotes, who leapfrogged idle Edmonton into first place in the Pacific Division. Arizona has won the first two contests of its four-game road trip and improved to 10-3-3 away from home this season.\n",
            "2021-11-20 17:33:10,986 - INFO - joeynmt.training - \tReference:  Ngibuke umdlalo, ngibe nomzuzu wenjabulo ephelele.\n",
            "2021-11-20 17:33:10,986 - INFO - joeynmt.training - \tHypothesis: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
            "2021-11-20 17:33:10,986 - INFO - joeynmt.training - Example #1\n",
            "2021-11-20 17:33:10,986 - INFO - joeynmt.training - \tSource:     One shows a girl next to her 20-year-old brother in military uniform.\n",
            "2021-11-20 17:33:10,986 - INFO - joeynmt.training - \tReference:  Bachaza ukuthi umoya esiwuphefumulayo uwagcwalisa kanjani amaphaphu ethu nge-oksijini.\n",
            "2021-11-20 17:33:10,986 - INFO - joeynmt.training - \tHypothesis: Uwwwwukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi ukuthi\n",
            "2021-11-20 17:33:10,987 - INFO - joeynmt.training - Example #2\n",
            "2021-11-20 17:33:10,987 - INFO - joeynmt.training - \tSource:     Yo Soy Muy Macho (1953) Silvia Pinal, Miguel Torruco.\n",
            "2021-11-20 17:33:10,987 - INFO - joeynmt.training - \tReference:  UConor Garland ufake igoli le-11 leqembu lakhe ebelihamba phambili ehlandleni lesithathu, bese uClayton Keller wazisa kabili kumaCoyote, ogxumise i-Edmonton yaba sendaweni yokuqala Ophikweni lakuPacific. I-Arizona inqobe imincintiswano emibili yokuqala ohambweni lwayo lwemidlalo emine futhi yathuthukela ku-10-3-3 engakho ekhaya kulesi sikhathi sokudlala.\n",
            "2021-11-20 17:33:10,987 - INFO - joeynmt.training - \tHypothesis: UAuAAAAAAAAAAAuSuSuSeeeeNNNNNNNNTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n",
            "2021-11-20 17:33:10,987 - INFO - joeynmt.training - Example #3\n",
            "2021-11-20 17:33:10,988 - INFO - joeynmt.training - \tSource:     On the other side, President Vladimir V. Putin of Russia, having promoted himself for years as a defender of Russia's reach as an Orthodox power, also has a keen political interest and desperately wants to keep Ukraine under the wing of the Moscow patriarch.\n",
            "2021-11-20 17:33:10,988 - INFO - joeynmt.training - \tReference:  Umuntu uveza intombazane eseduze kukamfowabo oneminyaka engama-20 ogqoke umfaniswano wamasosha.\n",
            "2021-11-20 17:33:10,988 - INFO - joeynmt.training - \tHypothesis: \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
            "2021-11-20 17:33:10,988 - INFO - joeynmt.training - Validation result (greedy) at epoch  26, step      500: bleu:   0.03, loss: 29080.8398, ppl: 417.6406, duration: 13.4104s\n",
            "2021-11-20 17:33:12,495 - INFO - joeynmt.training - Epoch  26: total training loss 107.72\n",
            "2021-11-20 17:33:12,495 - INFO - joeynmt.training - EPOCH 27\n",
            "2021-11-20 17:33:19,669 - INFO - joeynmt.training - Epoch  27: total training loss 106.94\n",
            "2021-11-20 17:33:19,670 - INFO - joeynmt.training - EPOCH 28\n",
            "2021-11-20 17:33:26,828 - INFO - joeynmt.training - Epoch  28: total training loss 106.18\n",
            "2021-11-20 17:33:26,829 - INFO - joeynmt.training - EPOCH 29\n",
            "2021-11-20 17:33:33,951 - INFO - joeynmt.training - Epoch  29: total training loss 105.41\n",
            "2021-11-20 17:33:33,952 - INFO - joeynmt.training - EPOCH 30\n",
            "2021-11-20 17:33:41,263 - INFO - joeynmt.training - Epoch  30: total training loss 110.29\n",
            "2021-11-20 17:33:41,264 - INFO - joeynmt.training - Training ended after  30 epochs.\n",
            "2021-11-20 17:33:41,264 - INFO - joeynmt.training - Best validation result (greedy) at step      500: 417.64 ppl.\n",
            "2021-11-20 17:33:41,288 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 3600\n",
            "2021-11-20 17:33:41,289 - INFO - joeynmt.prediction - Loading model from models/enzu_transformer/500.ckpt\n",
            "2021-11-20 17:33:41,502 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-11-20 17:33:41,753 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-11-20 17:33:41,829 - INFO - joeynmt.prediction - Decoding on dev set (data/enzu/dev.bpe.zu)...\n",
            "2021-11-20 17:34:28,846 - INFO - joeynmt.prediction -  dev bleu[none]:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-11-20 17:34:28,847 - INFO - joeynmt.prediction - Translations saved to: models/enzu_transformer/00000500.hyps.dev\n",
            "2021-11-20 17:34:28,847 - INFO - joeynmt.prediction - Decoding on test set (data/enzu/test.bpe.zu)...\n",
            "2021-11-20 17:51:19,592 - INFO - joeynmt.prediction - test bleu[none]:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-11-20 17:51:19,594 - INFO - joeynmt.prediction - Translations saved to: models/enzu_transformer/00000500.hyps.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBoDS09JM807"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistent storage \n",
        "! mkdir -p \"$gdrive_path/models/${src}${tgt}_transformer/\"\n",
        "! cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n94wlrCjVc17",
        "outputId": "c5b1351e-73d9-4992-a2ab-98bc5d84ebde"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps: 100\tLoss: 29641.04297\tPPL: 469.12527\tbleu: 0.01651\tLR: 0.00030000\t*\n",
            "Steps: 200\tLoss: 29303.06250\tPPL: 437.35040\tbleu: 0.00000\tLR: 0.00030000\t*\n",
            "Steps: 300\tLoss: 29319.38477\tPPL: 438.83441\tbleu: 0.01352\tLR: 0.00030000\t\n",
            "Steps: 400\tLoss: 29250.28711\tPPL: 432.58688\tbleu: 0.02931\tLR: 0.00030000\t*\n",
            "Steps: 500\tLoss: 29080.83984\tPPL: 417.64059\tbleu: 0.03035\tLR: 0.00030000\t*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66WhRE9lIhoD",
        "outputId": "b4a8dce1-7633-4fce-f6df-45d9bc67d2b5"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-20 17:58:17,707 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-11-20 17:58:17,708 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-11-20 17:58:17,977 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-11-20 17:58:17,980 - INFO - joeynmt.data - Loading test data...\n",
            "2021-11-20 17:58:18,012 - INFO - joeynmt.data - Data loaded.\n",
            "2021-11-20 17:58:18,038 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 3600\n",
            "2021-11-20 17:58:18,038 - INFO - joeynmt.prediction - Loading model from models/enzu_transformer/500.ckpt\n",
            "2021-11-20 17:58:20,820 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-11-20 17:58:21,078 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-11-20 17:58:21,162 - INFO - joeynmt.prediction - Decoding on dev set (data/enzu/dev.bpe.zu)...\n",
            "2021-11-20 17:59:09,102 - INFO - joeynmt.prediction -  dev bleu[none]:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-11-20 17:59:09,103 - INFO - joeynmt.prediction - Decoding on test set (data/enzu/test.bpe.zu)...\n",
            "2021-11-20 18:16:02,244 - INFO - joeynmt.prediction - test bleu[none]:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzhylnriTEYT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}